
\chapter[Darstellung der Grundlagen]{Darstellung der Grundlagen\\
\normalsize{(nicht selbstgemachtes
Secondo,
MovingRegion,
T\o{}ssebro)}\anmerkung{30-40 Seiten}} \label{Kapitel2}
\minitoc
\newpage
\section{Die Vorschl"age von Erlend T\o{}ssebro}
In seinem Paper nannte der Autor drei verschiedene Matching-Strategien:
\begin{enumerate}
\item Position of centroid

Bestimme den Schwerpunkt jedes Cycles,  bilde aus diesen einen gewichteten Graphen, mit den Entfernungen als Kantengewichte und suche in Diesem "`N"achste Nachbarn"'.
\item Fixed threshold (set of cycles)

Matche zwei Cycles, wenn sie sich wechselseitig  mehr als threshold (in \%) "uberlappen.

\item Maximize Overlap (set of cycles)

Bilde einen gewichteten Graphen, in dem die Cycles Knoten sind, und dessen Kanten mit dem Grad der "Uberlappung gewichtet sind. Matche dann ein Cycle $c$ mit demjenigen, mit dem er die gr"o"ste "Uberlappung aufweist, und mit allen, f"ur die $c$ der Cyclemit der gr"o"sten "Uberlappung ist.
\end{enumerate} 
\section{Meine Bewertung der Vorschl"age}
\begin{enumerate}
\item Position of centroid

In dem Paper wurde diese M"oglichkeit nicht weiter betrachtet, da er sagte, dass ein Referenzpunkt, der auch au"serhalb des Polygons liegen kann nicht gut sein kann. Hierzu muss ich anmerken, dass wir das Matching auf Ebene des ConvexHullTrees durchf"uhren, und das wir es hier nur mit konvexen H"ullen von Polygone zu tun haben. Der Schwerpunkt einer konvexen H"ulle liegt nat"urlich aber wieder innerhalb Dieser (wenngleich nicht zwangsl"aufig auch innerhalb des repr"asentierten Polygons). Au"serdem erscheint es plausibel, dass ein Referenzpunkt-Verfahren seine St"arke gerade dort haben wird, wo die "Uberlappungsalgorithmen ihre Schw"achen haben (bei sich schnell bewegenden, kleinen Objekten).

Der Vorschlag, "`N"achste Nachbarn"' zu benutzen, hat den Nachteil, dass man so nur 1:1 Matches finden kann. Statt dessen habe ich mich zu einem Schwellwert-Verfahren entschlossen, in dem zu einem Element der einen Seite, alle Elemente der Anderen gematcht werden, deren Schwerpunkte weniger als "`Schwellwert"' entfernt sind. 

\item Fixend threshold (set of cycles)

Zu diesem Verfahren kann ich eigentlich nicht viel sagen, ich habe es so implementiert, wie Erlend T\o{}ssebro das beschrieben hat, und es funktioniert.

\item Maximize Overlap (set of cycles)

Dieses Verfahren k"onnte interesannt sein, ist aber noch nicht implementiert.

\item Matching mit dem Steiner-Punkt

In dem Aufsatz \cite{AAR} wurde das Matching mit dem Steiner-Punkt als Referenzpunkt durchgef"uhrt. Auch wenn die Grundausrichtung dieser Arbeit von meiner verschieden ist, so ist es doch m"oglicherweise interessant, diese Spur weiterzuverfolgen. Zu dem eigentlichen Ablauf des Matchings gilt dasselbe, wie f"ur das Schwerpunktverfahren.
\end{enumerate}

\section{Matching mit Referenzpunkten}

Nehmen wir also an, wir haben eine M"oglichkeit, zu jedem Polygon, das wir matchen wollen einen Referenzpunkt zu berechnen. Dann k"onnen wir ein Feld aufbauen, in dem zu jeder Kombination von Cycles die Entfernung der Referenzpunkte eingetragen werden. Ordnen wir dieses Feld aufsteigend nach den Entfernungen, so steht zu erwarten, dass es zwischen den erw"unschten und den unerw"unschten Matches einen Sprung in dem Graphen geben wird. Diesen Sprung m"u"ste man mit geeigneten statistischen Verfahren finden k"onnen. Nachteilig an diesem Verfahren ist, dass die Laufzeit dieses Verfahrens hoch ist. Haben wir auf den beiden Seiten des Matchings $n$ beziehungsweise $m$ Referenzpunkte, so ist hat das resultierende Feld die Dimension $n\times m$. Dieses Feld kann man in $O(n\times m)$aufbauen. Zum Sortieren des Feldes braucht man dann $O((n\times m)\log(n\times m))$. Also ist dises Verfahren mindestens vom Typ $O((n\times m)\log(n\times m))$ (je nach dem verwendeten statistischen Verfahren k"onnte die Laufzeit sogar schlechter sein).

Wegen dieser schlechten Laufzeit, und weil ich noch kein geeignetes Verfahren gefunden habe, verwende ich das wesentlich einfachere Schwellwertverfahren, dass man formulieren kann: Matche zwei Cycle, wenn der Abstand ihrer Referenzpunkte kleiner als der Schwellwert ist. 

Die Laufzeit dieses Verfahrens ist wesentlich besser. Ich berechne zu jeder der $m\times n$ verschiedenen  Referenzpunkte den Abstand (in konstanter Zeit) und vergleiche diesen mit dem Schwellwert. Also ist die Laufzeit$O(n\times n)$.

Aufgrund der Verschiedenheit der m"oglichen Daten ist ein solcher absoluter Schwellwert $t_{abs}$ nat"urlich nicht handhabbar, so dass die Matchingfunktion mit einem relativen Schwellwert $t_{rel}$ (in \%) arbeiten muss. Aus Diesem wird dann intern ein absoluter Schwellwert berechnet, der dann wie oben verwendet wird. Man berechnent $t_{abs}$, indem man $t_{rel}$ mit dem gr"o"sten Abstand multipliziert, den die beiden Regionen voneinander haben. Leider ben"otigt man f"ur die Berechnung dieses Wertes $O(n\times m)$. Praktisch verwende ich desshalb einen Algorithmus, der mir in $O((n+m)\log{n+m})$ einen sehr "ahnlich Wert berechnet. Siehe hierzu: "`Spezielle Algorithmen"'.

\section{Die Wahl des Referenzpunktes}
In den Arbeiten \cite{AAR} und  \cite{AFRW} wurden der Schwerpunkt und der Steinerpunkt als m"ogliche Referenzpunkte gefunden. Ich fasse die Ergebnisse hier noch einmal zusammen:

\subsection{\index*{Hausdorff-Abstand}}

Seien $A$ und $B$ zwei kompackte Teilmangen des $\mathbb{R}^2$ und sei $\Vert\centerdot\Vert$ die Euklidische Norm.
Dann definieren wir eine Hilfsfunktion $ \widetilde{\delta_H}  $, den einseitige Hausdoff-Abstand, wie folgt:
\[ \widetilde{\delta_H}(A,B):=\max_{a\in A} \;\min_{b\in B} \Vert a-b \Vert\]
Man kann sehen, dass der einseitige Hausdorf-Abstand von Polygon $A$ zu $B$ der Abstand des am weitesten entfernten Punktes aus $A$ zu dem im am n"achsten gelegenen Punkt aus $B$ ist. Dann ist der Hausdorff-Abstand $\delta_H$ definiert als:
\[\delta_H:=\max\{\widetilde{\delta_H}(A,B),\widetilde{\delta_H}(B,A)\}.\]


\subsection{Die symmetrische Differenz}

In \cite{AFRW} wird vorgeschlagen den Fl"acheninhalt der symmetrischen Differenz als Abstands-Ma"s zweier konvexer Polygone zu benutzen. Die Symmetrische Differenz von zwei kompakten Teilmengen $A$ und $B$ des $\mathbb{R}^2 $ ist definiert als:
\[A\bigtriangleup B:=(A\setminus B)\cup(B\setminus A).\]
Wenn $A(\cdot)$ der Fl"acheninhalt ist, so bildet $\delta_S$ den Abstand nach der symmetrischen Differenz:
\[\delta_S:=A(A \bigtriangleup B).\]

\subsection{Der Schwerpunkt}

Der Schwerpunkt eines Polygones ist der Schwerpunkt der Massenverteilung, die entsteht, wenn man allen Eckpunkten die selbe Masse zuordnet. Er berechnet sich f"ur ein Polygon $P$ mit $n$ Eckpunkten $v_i$:
\[p_0(P)=\sum^n_{i=1}v_i \frac{1}{n}.\]

\subsection{Der Steiner-Punkt}

In \cite{Sch} wird der Steinerpunkt beschrieben: "`als Schwerpunkt der Massenverteilung, die bei einem konvexen Polygon duch Belegung der Ecken mit den "au"seren Winkeln als Massen [...] gegeben ist"'. Also kann man den Steiner-Punkt eines konvexen Polygones $P$, das aus den $n$ Eckpunkten $v_i$ besteht, berechen ($\alpha_i$ ist hierbei der Innenwinkel von $v_i$):
\[p_2(P)=\sum^n_{i=1}v_i (\pi-\alpha_i).\]

\section{Mehrere Matchings besser als eines}

Wenn man die bereits ausprobierten Matchings betrachtet, so stellt man fest, dass das Overlapping-Match besser auf Vereinigungen und Aufsplitterungen von Cycles reagiert, und dass das Schwerpunkt-Verfahren besser auf kleine und schnelle Cycles abgestimmt ist. 

Es scheint also ein verfolgenswerter Ansatz zu sein, beide Matches (und vielleicht noch andere, etwa Overlapping mit mehreren Schwellwerten), zu berechnen, und aus diesen das beste Matching zu bestimmen. 

Um die G"ute eines gegebenen Matchings zu bestimmen, habe ich folgende Verfahren erdacht. Damit die verschiedenen Bewertungen vergleichbar sind, habe ich diese auf 1 normiert.
\begin{itemize}
\item Die Overlapp-Bewertung: 

Ist der Duchschnitt der "Uberlappungen gro"s, so ist das gut.

Dieses Bewertungsverfahren korespondiert zwar direkt mit dem Overlaping-Match, da es aber in \cite{AFRW} verwendet wird, benutze ich dieses Verfahren. Die Berechnung l"auft so:

$$r_O=\frac{\sum_{m\in M} \frac{A_{overlap}}{A_{source}+A_{target}}}{|M|}$$

\item Die Schwerpunkt-Bewertung:

Ist die durchschnittliche Entfernung der zueinander gematchten Schwerpunkte gro"s, so ist das schlecht.

Da dieses Verfahren direckt mit dem Schwerpunkt-Match korespondiert, und ausserdem nach \cite{AFRW} nicht zu erwarten steht, dass dieses Verfahren eine deutlich andere Werte als die Overlap-Bewertung liefert, verfolge ich dieses nicht weiter.

\item Die Summen-Norm:

Sind die zueinander gematchten Fl"achen etwa gleich gro"s, so ist das gut. 

Es steht zu erwarten, dass zueinander geh"orige F"achen etwa gleich gross sind. Auch wenn sich eine Fl"ache in mehrere andere teilt, wird die Summe der Fl"acheninhalte in etwa so gro"s sein, wie der Fl"acheninhalt der Ursprungsfl"ache. Zur Normierung teile ich den Kleineren durch den gr"o"seren Fl"acheninhalt, und bekomme somit die Abweichung in Prozent. Die Berechnung l"auft so:

$$A_{target}=\sum_{t\in Target}A_t$$
$$r_A=\frac {\sum_{m\in M}\frac{\min{A_{source},A_{target}}}{\max{A_{source},A_{target}}}}{|M|}$$

\item Die Hausdorff-Norm:

In \cite{AAR} wird die,  oben bereits eingef"uhrte, Hausdorff-Norm benutzt, um Matchings zu bewerten. 

Bei der Implementierung dieser Norm stellt sich die Frage der Normierung. Mangels Alternative habe ich mich dazu entschlossen, alle einzelnen Hausdorff--Abst"ande duch den Duchmesser $d$ der Ausgangsregionen zu teilen.  Durchmesser bezeichne hierbei den Gr"o"sten Abstand, den zwei Punkte zueinander haben. Dieses Vorgehen bedeutet aber leider, dass die Abst"ande von kleinen Konkavit"aten relativ wenig in die Gesamtbewertung eingehen. Die Berechnung lautet:

$$r_H=\frac{\sum_{m\in M}\delta_H(m)}{|M|\times d}$$


\item Die Linear-Norm

Bei der Implementierung der obigen Verfahren zeigte sich, dass die Verfahren eine Tendenz zu "ubertriebenen Zersplitterungen aufweisen. Unter der Pr"amisse, dass dies in den allermeisten F"allen eher selten der Fall ist, f"urte ich diesen Bewertungsfaktor ein, um dieses Verhalten abzuschw"achen.

F"uhrt das Matching zu einer geringen Anzahl an Zersplitterungen, so ist das gut.

Die Berechnung l"auft so:




$$L(m)=
\begin{cases}
	\frac{1}{2} & \text{falls }|targets|=0\\
	\frac{1}{|targets|} & \text{sonst}
    \end{cases}
$$

$$r_L=\frac{\sum_{m\in M}L(m)}{|M|}$$

\item Die Struktur-Norm:

Stimmen die aufeinander gematchten Cycles strukturell weit "uberein, so ist das gut (eventuell kann man hier die Struktur der jeweiligen ConvexHullTrees heranziehen).

Diese Norm finde ich nach wie vor interesant, habe sie aber noch nicht weiter verfolgt. Nach der Betrachtung der Ergebnisse aus den anderen Normen, scheint eine weitere Verfolgung auch nicht notwendig. Zur Struktur der ConvexHullTrees ist zu sagen, dass in \cite{TG} der Umstand beschrieben ist , das "ahnliche Polygone teilweise recht unterschiedliche ConvexHullTree-Repr"asentationen aufweisen k"onnen, was eine solche Bewertung nat"urlich negativ beweinflussen kann\footnote{Dieses ist aber weniger schlimm als es klingt, Herr T\o{}ssebro macht in diesem Artikel plausiebel, dass dieses Problem eher ein Problem von kleinen, k"unstlichen Testdatens"atzen als von realen Daten ist}.


\end{itemize} 

Wie man am Beispiel der Summen-Norm sehen kann, gibt es mehr M"oglichkeiten zu bewerten, als es effiziente Matchings gibt. Das Summen-Matching, das lauten k"onnte: "`Matche ein Cycle mit  einer Menge von Anderen, wenn die Summe der Fl"acheninhalte fast gleich ist zu dem Fl"acheninhalt des einen Cycles"', entspricht dem Rucksack-Problem und ist daher nicht effizient l"osbar. Die Bewertung, wie gut ein gegebenes Matching ist, sollte mit diesem Verfahren aber effizient m"oglich sein.
%\printindex

